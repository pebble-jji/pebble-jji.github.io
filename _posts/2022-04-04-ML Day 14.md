---
layout : single
title : "4/4(월) ML day 14" 
---
## 2장 연습문제
1. SVM 회귀를 kernel = 'linear'(하이퍼파라미터 C를 바꿔가며)나 kernel = 'rbf'(하이퍼파라미터 C와 gamma를 바꿔가며) 등의 다양한 하이퍼 파라미터 설정으로 시도해보세요. 최상의 SVR모델은 무엇인가요?

```python
from sklearn.svm import SVR

svm_reg = SVR()
svm_reg.fit(housing_prepared,housing_labels)

from sklearn.model_selection import GridSearchCV


param_grid = [
              {'kernel': ['linear'], 'C': [1,5,25,125,625,3125]},
              {'kernel': ['rbf'], 'C':  [1,5,25,125,625,3125],
                'gamma': [0.01,0.1,1,10]},]

svm_reg = SVR()

grid_search = GridSearchCV(svm_reg,param_grid, cv = 5,
                           scoring = 'neg_mean_squared_error',
                           return_train_score = True)

grid_search.fit(housing_prepared, housing_labels)
```

```python
GridSearchCV(cv=5, estimator=SVR(),
             param_grid=[{'C': [1, 5, 25, 125, 625, 3125],
                          'kernel': ['linear']},
                         {'C': [1, 5, 25, 125, 625, 3125],
                          'gamma': [0.01, 0.1, 1, 10], 'kernel': ['rbf']}],
             return_train_score=True, scoring='neg_mean_squared_error')

neg_mse = grid_search.best_score_

grid_search.best_params_
```

```python
{'C': 3125, 'gamma': 0.1, 'kernel': 'rbf'}
```

2. GridSearchCV를 RandomizedSearchCV로 바꿔보세요.

```python
from sklearn.model_selection import RandomizedSearchCV


param_grid = [
              {'kernel': ['linear'], 'C': [1,5,25,125,625,3125]},
              {'kernel': ['rbf'], 'C':  [1,5,25,125,625,3125],
                'gamma': [0.01,0.1,1,10]},]

svm_reg = SVR()

rand_search = RandomizedSearchCV(svm_reg,param_grid, cv = 5,
                           scoring = 'neg_mean_squared_error',
                           return_train_score = True)

rand_search.fit(housing_prepared, housing_labels)
```
```python
RandomizedSearchCV(cv=5, estimator=SVR(),
                   param_distributions=[{'C': [1, 5, 25, 125, 625, 3125],
                                         'kernel': ['linear']},
                                        {'C': [1, 5, 25, 125, 625, 3125],
                                         'gamma': [0.01, 0.1, 1, 10],
                                         'kernel': ['rbf']}],
                   return_train_score=True, scoring='neg_mean_squared_error')
```

```python
neg_mse = rand_search.best_score_
rand_search.best_params_
```

## 3. 분류

### 1. MNIST

#### Import Data

```python
from sklearn.datasets import fetch_openml # mnist 데이터 열기
mnist = fetch_openml('mnist_784', version = 1)
mnist.keys()
```

```python
dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])
```

#### Data Shape

```python
X,y = mnist['data'], mnist['target']

print(X.shape)

print(y.shape)
```

```python
(70000, 784)
(70000,)
```

#### 데이터 보기

```python
import matplotlib as mpl
import matplotlib.pyplot as plt

some_digit = X[0] # 0번 그림 호출
some_digit_image = some_digit.reshape(28,28) # 픽셀에 불이 들어오는지 아닌지 

plt.imshow(some_digit_image, cmap = 'binary') # 이미지로 반영
plt.axis('off')
plt.show()
```

![img](data:image/png;base64,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)

5인 것으로 보인다.

```python
y = y.astype(np.uint8)
y[0]
```

타겟값도 5임을 알 수 있다.

#### Test Set 분리하기

```python
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
```

이미 섞여있기 때문에 굳이 섞지 않는다.



### 2. 이진 분류기 훈련

확률적 경사하강법(SGD)  
랜덤하게 추출한 일부 데이터에 대해 가중치를 조절
- 속도 빠름
- 최적 해의 정확도 낮음

$W(t+1) = W(t) -\alpha \frac{\partial}{∂w} Cost(w)$
> - Learning Rate : $\alpha$
> - Gradient : $ \frac{\partial}{∂w} Cost(w) $
>
> 조금씩 앞으로 접근하게 하여 최적해에 다가가는 방식

```python
y_train_5 = (y_train == 5) # boolean 데이터로 이뤄진 벡터
y_test_5 = (y_test == 5)
```

```python
from sklearn.linear_model import SGDClassifier

sgc = SGDClassifier(random_state = 42)  # 시드 유지
sgc.fit(X_train,y_train_5) # 훈련세트로 적합

sgc.predict([some_digit]) # 저 사진이 무엇인지 예측값 반환
```

```python
array([ True]) # 얘는 5가 맞을거라고 예상함
```

정답이 맞다.

### 3. 성능 측정

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = StratifiedKFold(n_splits = 3, random_state = 42, shuffle = True)

for train_index, test_index in skfolds.split(X_train,y_train_5) : 
  clone_clf = clone(sgc)
  X_train_folds = X_train[train_index]
  y_train_folds = y_train_5[train_index]
  X_test_folds = X_train[test_index]
  y_test_folds = y_train_5[test_index]

  clone_clf.fit(X_train_folds, y_train_folds)
  y_pred = clone_clf.predict(X_test_folds)
  n_correct = sum(y_pred == y_test_folds)
  print(n_correct / len(y_pred))
```

```python
0.9669 # 모델 1 : 정확도 96.7%
0.91625 # 모델 2 : 정확도 91.6%
0.96785 # 모델 3 : 정확도 96.8%
```

```python
from sklearn.model_selection import cross_val_score
cross_val_score(sgc,X_train,y_train_5,cv = 3,scoring = 'accuracy')
```

```python
array([0.95035, 0.96035, 0.9604 ])
```

```
from sklearn.base import BaseEstimator

class Never5Classifier(BaseEstimator) : 
  def fit(self,X,y=None) :
    return self
  def predict(self,X) : 
    return np.zeros((len(X),1), dtype = bool)
```

```python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgc,X_train,y_train_5,cv = 3)
```

